{
  "hash": "b078ec408532a87548e5313e749c2a67",
  "result": {
    "markdown": "---\ntitle: \"Benchmark results analysis\"\nauthor: \"[John Zobolas](https://github.com/bblodfon)\"\ndate: last-modified\ndescription: \"Explore the relationship between proper and improper Integrated Brier Score in the validation of survival models\"\nbibliography: references.bib\nformat:\n  html:\n    date: last-modified\n    code-block-bg: true\n    code-copy: true\n    code-overflow: wrap\n    code-block-border-left: true\n    toc: true\n    toc-location: left\n    html-math-method: katex\n    page-layout: full\nexecute: \n  freeze: true\n---\n\n\n\n\n## Introduction {-}\n\nWe benchmark the two versions of the survival brier score  [@Graf1999], namely the **Integrated Survival Brier Score (ISBS)** and the proposed **we-weighted version (RISBS)** (see [documentation details](https://mlr3proba.mlr-org.com/reference/mlr_measures_surv.graf.html#details) for their respective formulas).\nThe first (ISBS) is not a proper scoring rule [@Rindt2022], the second (RISBS) is [@Sonabend2022].\nOur goal is to assess whether these scores exhibit differences in simulated and real-world datasets, and if so, to understand the reasons behind these differences.\n\nLoad libraries:\n\n::: {.cell result='false'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(mlr3proba)\nlibrary(DT)\n```\n:::\n\n\n## Simulated Data Results {-}\n\n:::{.callout-note}\n- [Code](https://github.com/bblodfon/scoring-rules-2024/blob/main/generate.R) to generate the simulated datasets.\n\nWe simulate datasets with **varying characteristics**:\n\n1. Independent (random) vs dependent censoring\n2. PH (Proportional Hazards) vs non-PH data (time-varying coefficients)\n3. Proportion of censoring ($20\\% - 80\\%$)\n4. Number of observations ($100 - 1000$)\n\nThe 'fixed' parameters in our simulations are the following:\n\n- Time horizon (max event or censoring time): **365 days**\n- Number of datasets to generate per (1)-(4) combinations: $100$\n- Number of covariates per dataset (chosen randomly): $3-10$ (low-dim setting)\n:::\n\n## Real-world Data Results {-}\n\n:::{.callout-note}\n- [Compressed data files](https://github.com/bblodfon/scoring-rules-2024/tree/main/data)\n- [R script](https://github.com/bblodfon/scoring-rules-2024/blob/main/prepare_tasks.R) used to translate the datasets into `mlr3` tasks and extract useful info, namely:\n  - `n_obs`: Number of observations\n  - `n_vars`: Number of total variables\n  - `n_factors`: Number of factor/categorical variables\n  - `n_numeric`: Number of numeric variables\n  - `cens_prop`: Proportion of censoring\n  - `admin_cens_prop`: Proportion of censored observations that are censored administratively, i.e. at the last censoring time\n  - `dep_cens_prop`: Proportion of significant coefficients (adjusted `p < 0.05`) to predict censoring status using a logistic regression model\n  - `prop_haz`: If the dataset satisfies the proportional hazards assumption (`p > 0.05` using a global Schoenfeld test)\n:::\n\nWe used a total of $26$ real-word, low-dimensional datasets (fewer features than observations) for benchmarking, freely available via various `R` packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntask_tbl = readRDS(file = \"task_tbl.rds\") |> select(-task)\n\ndatatable(data = task_tbl, rownames = FALSE, options = \n  list(pageLength = 13, searching = FALSE,\n       order = list(list(0, 'asc')))) |>\n  formatRound(columns = 6:8, digits = 2) |>\n  formatStyle(columns = 'prop_haz',\n              backgroundColor = styleEqual(c(TRUE, FALSE), c(\"#4DAF4A\", \"#E41A1C\")))\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-8d582ee66b3d4da71707\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-8d582ee66b3d4da71707\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"aids.id\",\"aids2\",\"channing\",\"colrec\",\"cost\",\"dataFTR\",\"dataSTR\",\"e1684\",\"gbsg\",\"grace\",\"kidtran\",\"liver\",\"lung\",\"metabric\",\"mgus\",\"nafld1\",\"nwtco\",\"ova\",\"rdata\",\"std\",\"support\",\"tumor\",\"uis\",\"veteran\",\"wbc1\",\"whas\"],[467,2814,458,5578,518,2206,546,284,2232,1000,863,488,228,1903,176,4000,4028,358,1040,877,4000,776,575,137,190,481],[5,4,2,5,13,2,4,3,7,6,3,2,8,9,7,5,3,5,3,21,14,7,12,6,2,9],[4,2,0,2,10,0,0,0,0,0,0,0,1,0,1,1,1,3,1,4,0,1,1,3,0,2],[1,2,2,3,3,2,4,3,7,6,3,2,7,9,6,4,2,2,2,17,14,6,11,3,2,7],[0.5974304068522484,0.3841506751954513,0.6157205240174672,0.17497310864109,0.2200772200772201,0.8640072529465095,0.815018315018315,0.3098591549295774,0.432347670250896,0.676,0.8377752027809965,0.4016393442622951,0.2763157894736842,0.4203888596952181,0.0625,0.9195,0.8582423038728898,0.2569832402234637,0.4740384615384615,0.6043329532497149,0.32375,0.5167525773195877,0.1930434782608696,0.06569343065693431,0.4263157894736842,0.4823284823284824],[0.003584229390681004,0.0009250693802035153,0.4397163120567376,0.001024590163934426,0.008771929824561403,0.001049317943336831,0.002247191011235955,0.01136363636363636,0.001036269430051813,0.7248520710059172,0.001383125864453665,0.00510204081632653,0.01587301587301587,0.00125,0.09090909090909091,0.0002718868950516585,0.0002892681515765114,0.0108695652173913,0.002028397565922921,0.001886792452830189,0.002316602316602316,0.002493765586034913,0.009009009009009009,0.1111111111111111,0.01234567901234568,0.004310344827586207],[0.3333333333333333,0,0.6666666666666666,0.8333333333333334,0.1428571428571428,0.6666666666666666,0.6,0.25,0.5,0.7142857142857143,0.5,0.6666666666666666,0.1111111111111111,0.6,0.125,0.3333333333333333,1,0.07692307692307693,0.5714285714285714,0,0.2666666666666667,0.3076923076923077,0.06666666666666667,0,0,0.4666666666666667],[true,false,true,false,false,true,true,true,false,false,true,false,true,false,true,true,false,false,true,true,false,false,true,false,true,false]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>task_id<\\/th>\\n      <th>n_obs<\\/th>\\n      <th>n_vars<\\/th>\\n      <th>n_factors<\\/th>\\n      <th>n_numeric<\\/th>\\n      <th>cens_prop<\\/th>\\n      <th>admin_cens_prop<\\/th>\\n      <th>dep_cens_prop<\\/th>\\n      <th>prop_haz<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":13,\"searching\":false,\"order\":[[0,\"asc\"]],\"columnDefs\":[{\"targets\":5,\"render\":\"function(data, type, row, meta) {\\n    return type !== 'display' ? data : DTWidget.formatRound(data, 2, 3, \\\",\\\", \\\".\\\", null);\\n  }\"},{\"targets\":6,\"render\":\"function(data, type, row, meta) {\\n    return type !== 'display' ? data : DTWidget.formatRound(data, 2, 3, \\\",\\\", \\\".\\\", null);\\n  }\"},{\"targets\":7,\"render\":\"function(data, type, row, meta) {\\n    return type !== 'display' ? data : DTWidget.formatRound(data, 2, 3, \\\",\\\", \\\".\\\", null);\\n  }\"},{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7]},{\"name\":\"task_id\",\"targets\":0},{\"name\":\"n_obs\",\"targets\":1},{\"name\":\"n_vars\",\"targets\":2},{\"name\":\"n_factors\",\"targets\":3},{\"name\":\"n_numeric\",\"targets\":4},{\"name\":\"cens_prop\",\"targets\":5},{\"name\":\"admin_cens_prop\",\"targets\":6},{\"name\":\"dep_cens_prop\",\"targets\":7},{\"name\":\"prop_haz\",\"targets\":8}],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[10,13,25,50,100],\"rowCallback\":\"function(row, data, displayNum, displayIndex, dataIndex) {\\nvar value=data[8]; $(this.api().cell(row, 8).node()).css({'background-color':value == true ? \\\"#4DAF4A\\\" : value == false ? \\\"#E41A1C\\\" : null});\\n}\"}},\"evals\":[\"options.columnDefs.0.render\",\"options.columnDefs.1.render\",\"options.columnDefs.2.render\",\"options.rowCallback\"],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n## Investigate inflation of proper ISBS {-}\n\n:::{.callout-note}\nIn this section we investigate an example where the **proper ISBS gets inflated** (i.e. too large value for the score, compared to the improper version) and show how we can avoid such a thing from happening when evaluating model performance.\n:::\n\nLet's use a dataset where in a particular train/test resampling the issue occurs:\n\n::: {.cell}\n\n```{.r .cell-code}\ninflated_data = readRDS(file = \"inflated_data.rds\")\ntask = inflated_data$task\npart = inflated_data$part\n\ntask\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TaskSurv:mgus> (176 x 9)\n* Target: time, status\n* Properties: -\n* Features (7):\n  - dbl (6): age, alb, creat, dxyr, hgb, mspike\n  - fct (1): sex\n```\n:::\n:::\n\n\nSeparate train and test data:\n\n::: {.cell}\n\n```{.r .cell-code}\ntask_train = task$clone()$filter(rows = part$train)\ntask_test  = task$clone()$filter(rows = part$test)\n```\n:::\n\n\nKaplan-Meier of the training survival data:\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(task_train) +\n  labs(title = \"Kaplan-Meier (train data)\",\n       subtitle = \"Time-to-event distribution\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nKaplan-Meier of the training censoring data:\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(task_train, reverse = TRUE) +\n    labs(title = \"Kaplan-Meier (train data)\",\n         subtitle = \"Censoring distribution\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nEstimates of the censoring distribution $G_{KM}(t)$ (values from the above figure):\n\n::: {.cell}\n\n```{.r .cell-code}\nkm_train = task_train$kaplan(reverse = TRUE)\nkm_tbl = tibble(time = km_train$time, surv = km_train$surv)\ntail(km_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 Ã— 2\n   time  surv\n  <dbl> <dbl>\n1 12140 0.75 \n2 12313 0.625\n3 12319 0.5  \n4 12349 0.25 \n5 12689 0.125\n6 13019 0    \n```\n:::\n:::\n\n\n:::{.callout-important}\nAs we can see from the above figures and table, due to having *at least one censored observation at the last time point*, $G_{KM}(t_{max}) = 0$ for $t_{max} = 13019$.\n:::\n\nIs there an observation **on the test set** that has died (`status` = $1$) on that last time point (or after)?\n\n::: {.cell}\n\n```{.r .cell-code}\nmax_time = max(km_tbl$time) # max time point\n\ntest_times  = task_test$times()\ntest_status = task_test$status()\n\n# get the id of the observation in the test data\nid = which(test_times >= max_time & test_status == 1)\nid\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14\n```\n:::\n:::\n\n\nYes there is such observation!\n\nIn `mlr3proba` using `proper = TRUE` for the RISBS calculation, this observation will be weighted by $1/0$ according to the formula.\nPractically, to avoid division by zero, a small value `eps = 0.001` will be used.\n\nLet's train a simple Cox model on the train set and calculate its predictions on the test set:\n\n::: {.cell}\n\n```{.r .cell-code}\ncox = lrn(\"surv.coxph\")\np = cox$train(task, part$train)$predict(task, part$test)\n```\n:::\n\n\nWe calculate the ISBS (improper) and RISBS (proper) scores:\n\n::: {.cell}\n\n```{.r .cell-code}\ngraf_improper = msr(\"surv.graf\", proper = FALSE, id = \"graf.improper\")\ngraf_proper   = msr(\"surv.graf\", proper = TRUE,  id = \"graf.proper\")\np$score(graf_improper, task = task, train_set = part$train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ngraf.improper \n    0.1493429 \n```\n:::\n\n```{.r .cell-code}\np$score(graf_proper  , task = task, train_set = part$train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ngraf.proper \n   10.64584 \n```\n:::\n:::\n\n\nAs we can see there is **huge difference** between the two versions of the score.\nWe check the *per-observation* scores (integrated across all time points):\n\nObservation-wise RISBS scores:\n\n::: {.cell}\n\n```{.r .cell-code}\ngraf_proper$scores\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]   0.08994417   0.02854219   0.04214266   0.15578719   0.05364692\n [6]   0.12969150   0.06463256   0.32033549   2.43262450   0.11602432\n[11]   0.03228501   0.10172088   0.14652850 367.10227335   0.18004727\n[16]   0.21991511   0.09070024   0.03507389   0.19856844   0.07925747\n[21]   0.07732517   0.06982001   0.19468406   0.05267402   0.02419841\n[26]   0.17645640   0.07633691   0.04379196   0.07839955   0.06684222\n[31]   0.05457688   0.02874430   0.04071108   0.00000000   0.00000000\n```\n:::\n:::\n\n\nObservation-wise ISBS scores:\n\n::: {.cell}\n\n```{.r .cell-code}\ngraf_improper$scores\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.08994417 0.02854219 0.04214266 0.15578719 0.05364692 0.12969150\n [7] 0.06463256 0.32033549 0.62971109 0.11602432 0.03228501 0.10172088\n[13] 0.14652850 1.07969258 0.16743979 0.21991511 0.09070024 0.03507389\n[19] 0.19856844 0.07925747 0.07732517 0.06982001 0.19468406 0.05267402\n[25] 0.02419841 0.16199516 0.07633691 0.04379196 0.07839955 0.06684222\n[31] 0.05457688 0.02874430 0.04071108 0.03512466 0.46541333\n```\n:::\n:::\n\n\nIt is **the one observation that we identified earlier** that causes the inflation of the RISBS score - it's pretty much an outlier compared to all other values:\n\n::: {.cell}\n\n```{.r .cell-code}\ngraf_proper$scores[id]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 367.1023\n```\n:::\n:::\n\n\nBy setting `t_max` (time horizon to evaluate the measure up to) to the $95\\%$ quantile of the event times, we can solve the inflation problem of the proper RISBS score, since we will divide by a value larger than zero from the above table of $G_{KM}(t)$ values.\nThe `t_max` time point is:\n\n::: {.cell}\n\n```{.r .cell-code}\nt_max = as.integer(quantile(task_train$unique_event_times(), 0.95))\nt_max\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10080\n```\n:::\n:::\n\n\nIntegrating up to `t_max`, the proper RISBS score is:\n\n::: {.cell}\n\n```{.r .cell-code}\ngraf_proper_tmax = msr(\"surv.graf\", proper = TRUE, t_max = t_max)\np$score(graf_proper_tmax, task = task, train_set = part$train) # ISBS\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsurv.graf \n0.1436484 \n```\n:::\n:::\n\n\nThe score for the specific observation that had experienced the event at (or beyond) the latest training time point is now:\n\n::: {.cell}\n\n```{.r .cell-code}\ngraf_proper_tmax$scores[id]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.141502\n```\n:::\n:::\n\n\n:::{.callout-tip title=\"Suggestion when calculating time-integrated scoring rules\"}\nTo avoid the inflation of RISBS and generally have a more robust estimation of both RISBS and ISBS time-dependent scores, we advise to set the `t_max` argument (time horizon).\nThis can be either study-driven or based on a meaningful quantile of the distribution of (usually event) times in your dataset (e.g. $80\\%$).\n:::\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<link href=\"site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/datatables-binding-0.33/datatables.js\"></script>\n<script src=\"site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"site_libs/dt-core-1.13.6/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/dt-core-1.13.6/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/dt-core-1.13.6/js/jquery.dataTables.min.js\"></script>\n<link href=\"site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}