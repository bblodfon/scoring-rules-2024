---
title: "Benchmark results analysis"
author: "[John Zobolas](https://github.com/bblodfon)"
date: last-modified
description: "Explore the relationship between proper and improper Integrated Brier Score in the validation of survival models"
bibliography: references.bib
format:
  html:
    date: last-modified
    code-block-bg: true
    code-copy: true
    code-fold: true
    code-overflow: wrap
    code-block-border-left: true
    toc: true
    toc-location: left
    html-math-method: katex
    page-layout: full
execute:
  freeze: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

## Aim {-}

We benchmark the two versions of the survival brier score  [@Graf1999], namely the **Integrated Survival Brier Score (ISBS)** and the proposed **we-weighted version (RISBS)** (see [documentation details](https://mlr3proba.mlr-org.com/reference/mlr_measures_surv.graf.html#details) for their respective formulas).
The first (ISBS) is not a proper scoring rule [@Rindt2022], the second (RISBS) is [@Sonabend2022].
Our goal is to assess whether these scores exhibit differences in simulated and real-world datasets, and if so, to understand the reasons behind these differences.

Load libraries:
```{r, result=FALSE, message=FALSE}
library(tidyverse)
library(mlr3proba)
library(DT)
library(ggpubr)
library(ComplexHeatmap)
library(circlize)
```

## Simulated Data Results {-}

:::{.callout-note}
- [Code](https://github.com/bblodfon/scoring-rules-2024/blob/main/generate.R) to generate the simulated datasets.
For directly getting the `res.rds` object, contact the author via a GitHub issue.

We simulate datasets with **varying characteristics**:

1. Independent (random) vs dependent censoring
2. PH (Proportional Hazards) vs non-PH data (time-varying coefficients)
3. Proportion of censoring ($10\% - 80\%$)
4. Number of observations ($100 - 1000$)

The 'fixed' parameters in our simulations are the following:

- Time horizon (max event or censoring time): **365 days**
- Number of datasets to generate per (1)-(4) combinations: $100$
- Number of covariates per dataset (chosen randomly): $3-10$ (low-dim setting)
:::

### Introduction {-}

For each simulated dataset, we performed a simple train/test resampling (70%/30%).
Each resampling was stratified using the `status` variable so that the proportion of censoring remains the same in each respective train and test set.

We trained 3 models in each respective train set, namely the **Kaplan-Meier**, the **Cox Proportional Hazards** (CoxPH) model and an **Accelerated Failure Time** (AFT) model with Weibull distribution for the time-to-event output variable.
We tested the performance of each model in each respective test set using the **ISBS** and **RISBS** measures, integrating up to the $80\%$ quantile of the event times of each train set.

Get the benchmark results:
```{r, cache=TRUE}
res = readRDS(file = "res.rds")
```

:::{.callout-tip title="Organization of Results"}
We will divide the presentation of simulation data results in **4 sub-sections**, according to: 

1. Whether the simulated datasets were satisfying the proportional hazards assumption and 
2. Whether censoring was dependent or not from the survival outcome.
:::

### Prop. Hazards and Independent Censoring {-}

For each combo of number of observations (`n_obs`) and proportion of censoring (`cens_prop`) variables ($100$ simulated datasets per combo), we calculate the following summary stats between RISBS and ISBS: **Pearson correlation**, **mean absolute difference** and its standard deviation, **root mean square error (RMSE)**:
```{r}
res_ph_ind = 
  res |> 
  drop_na() |> # exclude few datasets where AFT prediction didn't work
  filter(prop_haz == TRUE, cens_dep == FALSE) |>
  group_by(n_obs, cens_prop) |>
  summarize(
    .groups = "drop",
    km_cor  = cor(km_proper, km_improper),
    cox_cor = cor(cox_proper, cox_improper),
    aft_cor = cor(aft_proper, aft_improper),
    km_diff_mean  = mean(abs(km_proper - km_improper)),
    cox_diff_mean = mean(abs(cox_proper - cox_improper)),
    aft_diff_mean = mean(abs(aft_proper - aft_improper)),
    km_diff_sd    = sd(abs(km_proper - km_improper)),
    cox_diff_sd   = sd(abs(cox_proper - cox_improper)),
    aft_diff_sd   = sd(abs(aft_proper - aft_improper)),
    km_rmse  = sqrt(mean((km_proper - km_improper)^2)),
    cox_rmse = sqrt(mean((cox_proper - cox_improper)^2)),
    aft_rmse = sqrt(mean((aft_proper - aft_improper)^2))
)

res_ph_ind |>
  datatable(
    rownames = FALSE,
    options = list(pageLength = 10, searching = TRUE)) |>
  formatRound(columns = 2:14, digits = c(1, rep(2,3), rep(3,9)))
```

Visualizing the RMSE between RISBS and ISBS:

::: {.panel-tabset}
#### Cox

```{r}
cox_mat = 
  res_ph_ind |> 
  select(n_obs, cens_prop, cox_rmse) |> 
  pivot_wider(names_from = cens_prop, values_from = cox_rmse) |> 
  arrange(desc(n_obs)) |>
  column_to_rownames(var = "n_obs") |> 
  as.matrix()

# color function
max_val = round(max(c(res_ph_ind$km_rmse, res_ph_ind$cox_rmse, res_ph_ind$aft_rmse)), digits = 3) + 0.001
col_fun = circlize::colorRamp2(c(0, max_val/2, max_val), c("#e0f3db", "#a8ddb5", "#43a2ca"))

Heatmap(cox_mat, name = "RMSE", cluster_rows = FALSE, cluster_columns = FALSE,
        column_title = "Censoring Proportion", column_title_side = "bottom",
        column_labels = paste0(seq(from = 10, to = 80, by = 10), "%"),
        column_names_rot = 0, column_names_centered = TRUE,
        row_names_side = "left", row_title = "#Observations", col = col_fun)
```

#### AFT
```{r}
aft_mat = 
  res_ph_ind |> 
  select(n_obs, cens_prop, aft_rmse) |> 
  pivot_wider(names_from = cens_prop, values_from = aft_rmse) |> 
  arrange(desc(n_obs)) |>
  column_to_rownames(var = "n_obs") |> 
  as.matrix()

Heatmap(aft_mat, name = "RMSE", cluster_rows = FALSE, cluster_columns = FALSE,
        column_title = "Censoring Proportion", column_title_side = "bottom",
        column_labels = paste0(seq(from = 10, to = 80, by = 10), "%"),
        column_names_rot = 0, column_names_centered = TRUE, col = col_fun,
        row_names_side = "left", row_title = "#Observations")
```

#### Kaplan-Meier
```{r}
km_mat = 
  res_ph_ind |> 
  select(n_obs, cens_prop, km_rmse) |> 
  pivot_wider(names_from = cens_prop, values_from = km_rmse) |> 
  arrange(desc(n_obs)) |>
  column_to_rownames(var = "n_obs") |> 
  as.matrix()

Heatmap(km_mat, name = "RMSE", cluster_rows = FALSE, cluster_columns = FALSE,
        column_title = "Censoring Proportion", column_title_side = "bottom",
        column_labels = paste0(seq(from = 10, to = 80, by = 10), "%"),
        column_names_rot = 0, column_names_centered = TRUE, col = col_fun,
        row_names_side = "left", row_title = "#Observations")
```
:::

### Prop. Hazards and Dependent Censoring {-}

For each combo of number of observations (`n_obs`) and proportion of censoring (`cens_prop`) variables ($100$ simulated datasets per combo), we calculate the following summary stats between RISBS and ISBS: **Pearson correlation**, **mean absolute difference** and its standard deviation, **root mean square error (RMSE)**:
```{r}
res_ph_dep = 
  res |> 
  drop_na() |> # exclude few datasets where AFT prediction didn't work
  filter(prop_haz == TRUE, cens_dep == TRUE) |>
  group_by(n_obs, cens_prop) |>
  summarize(
    .groups = "drop",
    km_cor  = cor(km_proper, km_improper),
    cox_cor = cor(cox_proper, cox_improper),
    aft_cor = cor(aft_proper, aft_improper),
    km_diff_mean  = mean(abs(km_proper - km_improper)),
    cox_diff_mean = mean(abs(cox_proper - cox_improper)),
    aft_diff_mean = mean(abs(aft_proper - aft_improper)),
    km_diff_sd    = sd(abs(km_proper - km_improper)),
    cox_diff_sd   = sd(abs(cox_proper - cox_improper)),
    aft_diff_sd   = sd(abs(aft_proper - aft_improper)),
    km_rmse  = sqrt(mean((km_proper - km_improper)^2)),
    cox_rmse = sqrt(mean((cox_proper - cox_improper)^2)),
    aft_rmse = sqrt(mean((aft_proper - aft_improper)^2))
)

res_ph_dep |>
  datatable(
    rownames = FALSE,
    options = list(pageLength = 10, searching = TRUE)) |>
  formatRound(columns = 2:14, digits = c(1, rep(2,3), rep(3,9)))
```

Visualizing the RMSE between RISBS and ISBS:

::: {.panel-tabset}
#### Cox
```{r}
cox_mat = 
  res_ph_dep |> 
  select(n_obs, cens_prop, cox_rmse) |> 
  pivot_wider(names_from = cens_prop, values_from = cox_rmse) |> 
  arrange(desc(n_obs)) |>
  column_to_rownames(var = "n_obs") |> 
  as.matrix()

# color function
max_val = round(max(c(res_ph_dep$km_rmse, res_ph_dep$cox_rmse, res_ph_dep$aft_rmse)), digits = 3) + 0.001
col_fun = circlize::colorRamp2(c(0, max_val/2, max_val), c("#e0f3db", "#a8ddb5", "#43a2ca"))

Heatmap(cox_mat, name = "RMSE", cluster_rows = FALSE, cluster_columns = FALSE,
        column_title = "Censoring Proportion", column_title_side = "bottom",
        column_labels = paste0(seq(from = 10, to = 80, by = 10), "%"),
        column_names_rot = 0, column_names_centered = TRUE, col = col_fun,
        row_names_side = "left", row_title = "#Observations")
```

#### AFT
```{r}
aft_mat = 
  res_ph_dep |> 
  select(n_obs, cens_prop, aft_rmse) |> 
  pivot_wider(names_from = cens_prop, values_from = aft_rmse) |> 
  arrange(desc(n_obs)) |>
  column_to_rownames(var = "n_obs") |> 
  as.matrix()

Heatmap(aft_mat, name = "RMSE", cluster_rows = FALSE, cluster_columns = FALSE,
        column_title = "Censoring Proportion", column_title_side = "bottom",
        column_labels = paste0(seq(from = 10, to = 80, by = 10), "%"),
        column_names_rot = 0, column_names_centered = TRUE, col = col_fun,
        row_names_side = "left", row_title = "#Observations")
```

#### Kaplan-Meier
```{r}
km_mat = 
  res_ph_dep |> 
  select(n_obs, cens_prop, km_rmse) |> 
  pivot_wider(names_from = cens_prop, values_from = km_rmse) |> 
  arrange(desc(n_obs)) |>
  column_to_rownames(var = "n_obs") |> 
  as.matrix()

Heatmap(km_mat, name = "RMSE", cluster_rows = FALSE, cluster_columns = FALSE,
        column_title = "Censoring Proportion", column_title_side = "bottom",
        column_labels = paste0(seq(from = 10, to = 80, by = 10), "%"),
        column_names_rot = 0, column_names_centered = TRUE, col = col_fun,
        row_names_side = "left", row_title = "#Observations")
```
:::

### Non-Prop. Hazards and Independent Censoring {-}

For each combo of number of observations (`n_obs`) and proportion of censoring (`cens_prop`) variables ($100$ simulated datasets per combo), we calculate the following summary stats between RISBS and ISBS: **Pearson correlation**, **mean absolute difference** and its standard deviation, **root mean square error (RMSE)**:
```{r}
res_noph_ind = 
  res |> 
  drop_na() |> # exclude few datasets where AFT prediction didn't work
  filter(prop_haz == FALSE, cens_dep == FALSE) |>
  group_by(n_obs, cens_prop) |>
  summarize(
    .groups = "drop",
    km_cor  = cor(km_proper, km_improper),
    cox_cor = cor(cox_proper, cox_improper),
    aft_cor = cor(aft_proper, aft_improper),
    km_diff_mean  = mean(abs(km_proper - km_improper)),
    cox_diff_mean = mean(abs(cox_proper - cox_improper)),
    aft_diff_mean = mean(abs(aft_proper - aft_improper)),
    km_diff_sd    = sd(abs(km_proper - km_improper)),
    cox_diff_sd   = sd(abs(cox_proper - cox_improper)),
    aft_diff_sd   = sd(abs(aft_proper - aft_improper)),
    km_rmse  = sqrt(mean((km_proper - km_improper)^2)),
    cox_rmse = sqrt(mean((cox_proper - cox_improper)^2)),
    aft_rmse = sqrt(mean((aft_proper - aft_improper)^2))
)

res_noph_ind |>
  datatable(
    rownames = FALSE,
    options = list(pageLength = 10, searching = TRUE)) |>
  formatRound(columns = 2:14, digits = c(1, rep(2,3), rep(3,9)))
```

Visualizing the RMSE between RISBS and ISBS:

::: {.panel-tabset}
#### Cox
```{r}
cox_mat = 
  res_noph_ind |> 
  select(n_obs, cens_prop, cox_rmse) |> 
  pivot_wider(names_from = cens_prop, values_from = cox_rmse) |> 
  arrange(desc(n_obs)) |>
  column_to_rownames(var = "n_obs") |> 
  as.matrix()

# color function
max_val = round(max(c(res_noph_ind$km_rmse, res_noph_ind$cox_rmse, res_noph_ind$aft_rmse)), digits = 3) + 0.001
col_fun = circlize::colorRamp2(c(0, max_val/2, max_val), c("#e0f3db", "#a8ddb5", "#43a2ca"))

Heatmap(cox_mat, name = "RMSE", cluster_rows = FALSE, cluster_columns = FALSE,
        column_title = "Censoring Proportion", column_title_side = "bottom",
        column_labels = paste0(seq(from = 10, to = 80, by = 10), "%"),
        column_names_rot = 0, column_names_centered = TRUE, col = col_fun,
        row_names_side = "left", row_title = "#Observations")
```

#### AFT
```{r}
aft_mat = 
  res_noph_ind |> 
  select(n_obs, cens_prop, aft_rmse) |> 
  pivot_wider(names_from = cens_prop, values_from = aft_rmse) |> 
  arrange(desc(n_obs)) |>
  column_to_rownames(var = "n_obs") |> 
  as.matrix()

Heatmap(aft_mat, name = "RMSE", cluster_rows = FALSE, cluster_columns = FALSE,
        column_title = "Censoring Proportion", column_title_side = "bottom",
        column_labels = paste0(seq(from = 10, to = 80, by = 10), "%"),
        column_names_rot = 0, column_names_centered = TRUE, col = col_fun,
        row_names_side = "left", row_title = "#Observations")
```

#### Kaplan-Meier
```{r}
km_mat = 
  res_noph_ind |> 
  select(n_obs, cens_prop, km_rmse) |> 
  pivot_wider(names_from = cens_prop, values_from = km_rmse) |> 
  arrange(desc(n_obs)) |>
  column_to_rownames(var = "n_obs") |> 
  as.matrix()

Heatmap(km_mat, name = "RMSE", cluster_rows = FALSE, cluster_columns = FALSE,
        column_title = "Censoring Proportion", column_title_side = "bottom",
        column_labels = paste0(seq(from = 10, to = 80, by = 10), "%"),
        column_names_rot = 0, column_names_centered = TRUE, col = col_fun,
        row_names_side = "left", row_title = "#Observations")
```
:::

### Non-Prop. Hazards and Dependent Censoring {-}

For each combo of number of observations (`n_obs`) and proportion of censoring (`cens_prop`) variables ($100$ simulated datasets per combo), we calculate the following summary stats between RISBS and ISBS: **Pearson correlation**, **mean absolute difference** and its standard deviation, **root mean square error (RMSE)**:
```{r}
res_noph_dep = 
  res |> 
  drop_na() |> # exclude few datasets where AFT prediction didn't work
  filter(prop_haz == FALSE, cens_dep == TRUE) |>
  group_by(n_obs, cens_prop) |>
  summarize(
    .groups = "drop",
    km_cor  = cor(km_proper, km_improper),
    cox_cor = cor(cox_proper, cox_improper),
    aft_cor = cor(aft_proper, aft_improper),
    km_diff_mean  = mean(abs(km_proper - km_improper)),
    cox_diff_mean = mean(abs(cox_proper - cox_improper)),
    aft_diff_mean = mean(abs(aft_proper - aft_improper)),
    km_diff_sd    = sd(abs(km_proper - km_improper)),
    cox_diff_sd   = sd(abs(cox_proper - cox_improper)),
    aft_diff_sd   = sd(abs(aft_proper - aft_improper)),
    km_rmse  = sqrt(mean((km_proper - km_improper)^2)),
    cox_rmse = sqrt(mean((cox_proper - cox_improper)^2)),
    aft_rmse = sqrt(mean((aft_proper - aft_improper)^2))
)

res_noph_dep |>
  datatable(
    rownames = FALSE,
    options = list(pageLength = 10, searching = TRUE)) |>
  formatRound(columns = 2:14, digits = c(1, rep(2,3), rep(3,9)))
```

Visualizing the RMSE between RISBS and ISBS:

::: {.panel-tabset}
#### Cox
```{r}
cox_mat = 
  res_noph_dep |> 
  select(n_obs, cens_prop, cox_rmse) |> 
  pivot_wider(names_from = cens_prop, values_from = cox_rmse) |> 
  arrange(desc(n_obs)) |>
  column_to_rownames(var = "n_obs") |> 
  as.matrix()

# color function
max_val = round(max(c(res_noph_dep$km_rmse, res_noph_dep$cox_rmse, res_noph_dep$aft_rmse)), digits = 3) + 0.001
col_fun = circlize::colorRamp2(c(0, max_val/2, max_val), c("#e0f3db", "#a8ddb5", "#43a2ca"))

Heatmap(cox_mat, name = "RMSE", cluster_rows = FALSE, cluster_columns = FALSE,
        column_title = "Censoring Proportion", column_title_side = "bottom",
        column_labels = paste0(seq(from = 10, to = 80, by = 10), "%"),
        column_names_rot = 0, column_names_centered = TRUE, col = col_fun,
        row_names_side = "left", row_title = "#Observations")
```

#### AFT
```{r}
aft_mat = 
  res_noph_dep |> 
  select(n_obs, cens_prop, aft_rmse) |> 
  pivot_wider(names_from = cens_prop, values_from = aft_rmse) |> 
  arrange(desc(n_obs)) |>
  column_to_rownames(var = "n_obs") |> 
  as.matrix()

Heatmap(aft_mat, name = "RMSE", cluster_rows = FALSE, cluster_columns = FALSE,
        column_title = "Censoring Proportion", column_title_side = "bottom",
        column_labels = paste0(seq(from = 10, to = 80, by = 10), "%"),
        column_names_rot = 0, column_names_centered = TRUE, col = col_fun,
        row_names_side = "left", row_title = "#Observations")
```

#### Kaplan-Meier
```{r}
km_mat = 
  res_noph_dep |> 
  select(n_obs, cens_prop, km_rmse) |> 
  pivot_wider(names_from = cens_prop, values_from = km_rmse) |> 
  arrange(desc(n_obs)) |>
  column_to_rownames(var = "n_obs") |> 
  as.matrix()

Heatmap(km_mat, name = "RMSE", cluster_rows = FALSE, cluster_columns = FALSE,
        column_title = "Censoring Proportion", column_title_side = "bottom",
        column_labels = paste0(seq(from = 10, to = 80, by = 10), "%"),
        column_names_rot = 0, column_names_centered = TRUE, col = col_fun,
        row_names_side = "left", row_title = "#Observations")
```
:::

## Real-world Data Results {-}

:::{.callout-note}
- [Compressed data files](https://github.com/bblodfon/scoring-rules-2024/tree/main/data)
- [R script](https://github.com/bblodfon/scoring-rules-2024/blob/main/prepare_tasks.R) used to translate the datasets into `mlr3` tasks and extract useful info, namely:
  - `n_obs`: Number of observations
  - `n_vars`: Number of total variables
  - `n_factors`: Number of factor/categorical variables
  - `n_numeric`: Number of numeric variables
  - `cens_prop`: Proportion of censoring
  - `admin_cens_prop`: Proportion of censored observations that are censored administratively, i.e. at the last censoring time
  - `dep_cens_prop`: Proportion of significant coefficients (adjusted `p < 0.05`) to predict censoring status using a logistic regression model
  - `prop_haz`: If the dataset satisfies the proportional hazards assumption (`p > 0.05` using the Grambsch-Therneau test [@Grambsch1994])
- [R script](https://github.com/bblodfon/scoring-rules-2024/blob/main/run_bench.R) used to run the benchmark
:::

### Introduction {-}

We used a total of $26$ real-word, low-dimensional datasets (fewer features than observations) for benchmarking, some of which are freely available via various `R` packages.
For each dataset, we performed a simple train/test resampling (80%/20%) $100$ times.
Each resampling was stratified using the `status` variable so that the proportion of censoring remains the same in each respective train and test set.

We trained 3 models in each respective train set, namely the **Kaplan-Meier**, the **Cox Proportional Hazards** (CoxPH) model and an **Accelerated Failure Time** (AFT) model with Weibull distribution for the time-to-event output variable.
We tested the performance of each model in each respective test set using the **ISBS** and **RISBS** measures, integrating up to the $80\%$ quantile of the event times of each train set.
We kept also the brier scores for each specific observation (*per-observation* scores) in all respective test sets.

### Datasets table {-}

```{r}
# task info, see `prepare_tasks.R`
task_tbl = readRDS(file = "task_tbl.rds")

task_tbl |> 
  select(-task) |>
  datatable(
    rownames = FALSE, 
    options = list(pageLength = 13, searching = TRUE,
                   order = list(list(0, 'asc')))) |>
    formatRound(columns = 6:8, digits = 2) |>
    formatStyle(columns = 'prop_haz',
                backgroundColor = styleEqual(c(TRUE, FALSE), c("#4DAF4A", "#E41A1C")))
```

### Benchmarking stats {-}

Get the benchmark results:
```{r}
# see `run_bench.R`
# Compressed file: https://github.com/bblodfon/scoring-rules-2024/blob/main/bench_res.rds
bench_res = readRDS(file = "bench_res.rds")
```

We calculate the **Pearson correlation** and **root mean square error (RMSE)** between RISBS and ISBS scores per dataset ($100$ resamplings) and model:
```{r, message=FALSE}
score_corrs = 
  bench_res |> 
  group_by(task_id) |> 
  select(ends_with("proper")) |> 
  summarize(
    km_cor  = cor(km_proper, km_improper),
    cox_cor = cor(cox_proper, cox_improper),
    aft_cor = cor(aft_proper, aft_improper),
    km_rmse  = sqrt(mean((km_proper - km_improper)^2)),
    cox_rmse = sqrt(mean((cox_proper - cox_improper)^2)),
    aft_rmse = sqrt(mean((aft_proper - aft_improper)^2))
  ) |> 
  arrange(desc(cox_cor))

score_corrs |>
  datatable(
    rownames = FALSE,
    options = list(pageLength = 13, searching = TRUE, order = list(list(2, 'desc')))
  ) |>
  formatRound(columns = 2:7, digits = 3)
```

Mean and standard deviation of the Pearson's correlation and RMSE across all datasets:
```{r}
score_corrs |>
  summarise(across(-task_id, list(mean = mean, sd = sd))) |> 
  pivot_longer(cols = everything(), 
               names_to = c("model", "statistic", ".value"), 
               names_pattern = "(.*)_(.*)_(.*)"
  ) |>
  knitr::kable(digits = 3)
```

### RISBS vs ISBS {-}

Scatter plots of RISBS vs ISBS scores per dataset ($100$ dots/resamplings per figure).
Datasets are ordered by decreasing correlation between the two metrics:

```{r, cache=TRUE}
#| fig-width: 8
#| fig-height: 4

# order is by decreasing correlation
for (id in score_corrs$task_id) {
  p = 
    bench_res |>
    filter(task_id == id) |>
    select(ends_with("proper")) |>
    pivot_longer(cols = everything(), names_to = c("model", ".value"), names_pattern = "(.*)_(.*)") |>
    mutate(model = factor(model, levels = c("km", "cox", "aft"))) |>
    # scatter plot with Pearson's coef.
    ggpubr::ggscatter(
      x = "proper", y = "improper",
      facet.by = c("model"),
      panel.labs = list(model = c("Kaplan-Meier", "CoxPH", "AFT (Weibull)")),
      xlab = "RISBS (proper)",
      ylab = "ISBS (improper)",
      color = "black", shape = 21, size = 2,
      add = "reg.line",  # Add regression line
      add.params = list(color = "blue", fill = "lightgray"), # Customize regr. line
      conf.int = TRUE, # Add confidence interval
      cor.coef = TRUE, # Add Pearson's correlation coefficient
      cor.coeff.args = list(method = "pearson", label.sep = "\n")
    ) +
    labs(title = id) +
    theme(panel.spacing = unit(1, "cm"))
  print(p)
}
```

### RISBS vs ISBS (per-observation scores) {-}

Scatter plots of RISBS vs ISBS *per-observation* scores per dataset.
Every figure has a total of dots equal to ($100$ resamplings) x (number of test observations in each resampling).
Datasets are ordered by *decreasing proportion of censoring* after applying the $80\%$ quantile `t_max` cutoff in each full dataset.

```{r}
cens_props = vapply(task_tbl$task, function(task) {
  event_times = task$unique_event_times()
  t_max = unname(quantile(event_times, probs = 0.8))
  truth = task$truth()
  times  = truth[, 1]
  status = truth[, 2]
  status_tmax = status[times <= t_max]
  sum(status_tmax == 0)/length(status_tmax) # censoring proportion
}, numeric(1))

names(cens_props) = task_tbl$task_id
```

```{r, cache=TRUE, warning=FALSE}
#| fig-width: 8
#| fig-height: 4

ids = names(sort(cens_props, decreasing = TRUE))

for (id in ids) {
  p = 
    bench_res |>
    filter(task_id == id) |>
    select(ends_with("scores"), test_status) |> # `test_status` is the censoring status for coloring
    unnest(cols = everything()) |>
    pivot_longer(cols = ends_with("scores"), names_to = c("model", "type", ".value"), names_pattern = "(.*)_(.*)_(.*)") |>
    pivot_wider(names_from = type, values_from = scores, values_fn = list) |> 
    unnest(cols = everything()) |>
    rename(Status = test_status) |>
    mutate(Status = case_when(Status == 0 ~ "Censored", TRUE ~ "Event")) |>
    mutate(
      Status = as.factor(Status),
      model = factor(model, levels = c("km", "cox", "aft"))
    ) |>
    # scatter plot with Pearson's coef.
    ggpubr::ggscatter(
      x = "proper", y = "improper",
      facet.by = c("model"),
      panel.labs = list(model = c("Kaplan-Meier", "CoxPH", "AFT (Weibull)")),
      xlab = "RISBS (proper)",
      ylab = "ISBS (improper)",
      color = "Status", shape = 21, size = 2, alpha = 0.8,
      palette = c("Censored" = "#377eb8", "Event" = "#e41a1c"),
      add = "reg.line",  # Add regression line
      add.params = list(color = "blue", fill = "lightgray"), # Customize regr. line
      conf.int = TRUE, # Add confidence interval
      cor.coef = TRUE, # Add Pearson's correlation coefficient
      cor.coeff.args = list(method = "pearson", label.sep = "\n")
    ) +
    labs(title = id) +
    theme(panel.spacing = unit(1, "cm"))
  print(p)
}
```

Note that in the 4 last datasets we have censored observations with $t > t_{max}$ so they are excluded from the calculation of the brier scores as we integrate up to the `t_max` time horizon cutoff.
Therefore only observations that have experienced the event contribute to the scores (but the estimation of the censoring distribution using the Kaplan-Meier uses all observations of a train set).

## Investigate inflation of proper ISBS {-}

:::{.callout-note}
In this section we investigate an example where the **proper ISBS gets inflated** (i.e. too large value for the score, compared to the improper version) and show how we can avoid such a thing from happening when evaluating model performance.
:::

Let's use a dataset where in a particular train/test resampling the issue occurs:
```{r}
inflated_data = readRDS(file = "inflated_data.rds")
task = inflated_data$task
part = inflated_data$part

task
```

Separate train and test data:
```{r}
task_train = task$clone()$filter(rows = part$train)
task_test  = task$clone()$filter(rows = part$test)
```

Kaplan-Meier of the training survival data:
```{r, message=FALSE, cache=TRUE}
autoplot(task_train) +
  labs(title = "Kaplan-Meier (train data)",
       subtitle = "Time-to-event distribution")
```

Kaplan-Meier of the training censoring data:
```{r, message=FALSE, cache=TRUE}
autoplot(task_train, reverse = TRUE) +
    labs(title = "Kaplan-Meier (train data)",
         subtitle = "Censoring distribution")
```

Estimates of the censoring distribution $G_{KM}(t)$ (values from the above figure):
```{r}
km_train = task_train$kaplan(reverse = TRUE)
km_tbl = tibble(time = km_train$time, surv = km_train$surv)
tail(km_tbl)
```

:::{.callout-important}
As we can see from the above figures and table, due to having *at least one censored observation at the last time point*, $G_{KM}(t_{max}) = 0$ for $t_{max} = 13019$.
:::

Is there an observation **on the test set** that has died (`status` = $1$) on that last time point (or after)?
```{r}
max_time = max(km_tbl$time) # max time point

test_times  = task_test$times()
test_status = task_test$status()

# get the id of the observation in the test data
id = which(test_times >= max_time & test_status == 1)
id
```

Yes there is such observation!

In `mlr3proba` using `proper = TRUE` for the RISBS calculation, this observation will be weighted by $1/0$ according to the formula.
Practically, to avoid division by zero, a small value `eps = 0.001` will be used.

Let's train a simple Cox model on the train set and calculate its predictions on the test set:
```{r}
cox = lrn("surv.coxph")
p = cox$train(task, part$train)$predict(task, part$test)
```

We calculate the ISBS (improper) and RISBS (proper) scores:
```{r}
graf_improper = msr("surv.graf", proper = FALSE, id = "graf.improper")
graf_proper   = msr("surv.graf", proper = TRUE,  id = "graf.proper")
p$score(graf_improper, task = task, train_set = part$train)
p$score(graf_proper, task = task, train_set = part$train)
```

As we can see there is **huge difference** between the two versions of the score.
We check the *per-observation* scores (integrated across all time points):

Observation-wise RISBS scores:
```{r}
graf_proper$scores
```

Observation-wise ISBS scores:
```{r}
graf_improper$scores
```

It is **the one observation that we identified earlier** that causes the inflation of the RISBS score - it's pretty much an outlier compared to all other values:
```{r}
graf_proper$scores[id]
```

By setting `t_max` (time horizon to evaluate the measure up to) to the $95\%$ quantile of the event times, we can solve the inflation problem of the proper RISBS score, since we will divide by a value larger than zero from the above table of $G_{KM}(t)$ values.
The `t_max` time point is:
```{r}
t_max = as.integer(quantile(task_train$unique_event_times(), 0.95))
t_max
```

Integrating up to `t_max`, the proper RISBS score is:
```{r}
graf_proper_tmax = msr("surv.graf", id = "graf.proper", proper = TRUE, t_max = t_max)
p$score(graf_proper_tmax, task = task, train_set = part$train) # ISBS
```

The score for the specific observation that had experienced the event at (or beyond) the latest training time point is now:
```{r}
graf_proper_tmax$scores[id]
```

:::{.callout-tip title="Suggestion when calculating time-integrated scoring rules"}
To avoid the inflation of RISBS and generally have a more robust estimation of both RISBS and ISBS scoring rules, we advise to set the `t_max` argument (time horizon).
This can be either study-driven or based on a meaningful quantile of the distribution of (usually event) times in your dataset (e.g. $80\%$).
:::

## References
