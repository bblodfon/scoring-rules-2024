[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Benchmark results analysis",
    "section": "",
    "text": "We benchmark the two versions of the survival brier score (Graf et al. 1999), namely the Integrated Survival Brier Score (ISBS) and the proposed we-weighted version (RISBS) (see documentation details for their respective formulas). The first (ISBS) is not a proper scoring rule (Rindt et al. 2022), the second (RISBS) is (Sonabend 2022). Our goal is to assess whether these scores exhibit differences in simulated and real-world datasets, and if so, to understand the reasons behind these differences.\nLoad libraries:\n\n\nCode\nlibrary(tidyverse)\nlibrary(mlr3proba)\nlibrary(DT)\nlibrary(ggpubr)"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Benchmark results analysis",
    "section": "",
    "text": "We benchmark the two versions of the survival brier score (Graf et al. 1999), namely the Integrated Survival Brier Score (ISBS) and the proposed we-weighted version (RISBS) (see documentation details for their respective formulas). The first (ISBS) is not a proper scoring rule (Rindt et al. 2022), the second (RISBS) is (Sonabend 2022). Our goal is to assess whether these scores exhibit differences in simulated and real-world datasets, and if so, to understand the reasons behind these differences.\nLoad libraries:\n\n\nCode\nlibrary(tidyverse)\nlibrary(mlr3proba)\nlibrary(DT)\nlibrary(ggpubr)"
  },
  {
    "objectID": "index.html#simulated-data-results",
    "href": "index.html#simulated-data-results",
    "title": "Benchmark results analysis",
    "section": "Simulated Data Results",
    "text": "Simulated Data Results\n\n\n\n\n\n\nNote\n\n\n\n\nCode to generate the simulated datasets.\n\nWe simulate datasets with varying characteristics:\n\nIndependent (random) vs dependent censoring\nPH (Proportional Hazards) vs non-PH data (time-varying coefficients)\nProportion of censoring (20\\% - 80\\%)\nNumber of observations (100 - 1000)\n\nThe ‘fixed’ parameters in our simulations are the following:\n\nTime horizon (max event or censoring time): 365 days\nNumber of datasets to generate per (1)-(4) combinations: 100\nNumber of covariates per dataset (chosen randomly): 3-10 (low-dim setting)"
  },
  {
    "objectID": "index.html#real-world-data-results",
    "href": "index.html#real-world-data-results",
    "title": "Benchmark results analysis",
    "section": "Real-world Data Results",
    "text": "Real-world Data Results\n\n\n\n\n\n\nNote\n\n\n\n\nCompressed data files\nR script used to translate the datasets into mlr3 tasks and extract useful info, namely:\n\nn_obs: Number of observations\nn_vars: Number of total variables\nn_factors: Number of factor/categorical variables\nn_numeric: Number of numeric variables\ncens_prop: Proportion of censoring\nadmin_cens_prop: Proportion of censored observations that are censored administratively, i.e. at the last censoring time\ndep_cens_prop: Proportion of significant coefficients (adjusted p &lt; 0.05) to predict censoring status using a logistic regression model\nprop_haz: If the dataset satisfies the proportional hazards assumption (p &gt; 0.05 using a global Schoenfeld test)\n\n\n\n\nWe used a total of 26 real-word, low-dimensional datasets (fewer features than observations) for benchmarking, freely available via various R packages:\n\n\nCode\n# task info, see `prepare_tasks.R`\ntask_tbl = readRDS(file = \"task_tbl.rds\")\n\ntask_tbl |&gt; \n  select(-task) |&gt;\n  datatable(\n    rownames = FALSE, \n    options = list(pageLength = 13, searching = FALSE,\n                   order = list(list(0, 'asc')))) |&gt;\n    formatRound(columns = 6:8, digits = 2) |&gt;\n    formatStyle(columns = 'prop_haz',\n    backgroundColor = styleEqual(c(TRUE, FALSE), \n                                 c(\"#4DAF4A\", \"#E41A1C\")))\n\n\n\n\n\n\n\nGet the benchmark results:\n\n\nCode\n# see `run_bench.R`\nbench_res = readRDS(file = \"bench_res_tmax.rds\")\n\n\nOrder datasets by their Pearson correlation between RISBS and ISBS:\n\n\nCode\nscore_corrs = \n  bench_res |&gt; \n  group_by(task_id) |&gt; \n  select(ends_with(\"proper\")) |&gt; \n  summarize(\n    km_cor  = cor(km_proper, km_improper),\n    cox_cor = cor(cox_proper, cox_improper),\n    aft_cor = cor(aft_proper, aft_improper)\n  ) |&gt; \n  rowwise() |&gt; \n  mutate(mean_cor = mean(c_across(ends_with(\"cor\")), na.rm = TRUE)) |&gt; \n  ungroup() |&gt; \n  arrange(desc(mean_cor))\n\ntask_ids = score_corrs$task_id\n\n\nLoop through the tasks and compare RISBS and ISBS scores:\n\n\nCode\nfor (id in task_ids) {\n  p = bench_res |&gt;\n    filter(task_id == id) |&gt;\n    select(km_proper, km_improper, cox_proper, cox_improper, aft_proper, aft_improper) |&gt;\n    pivot_longer(cols = everything(), names_to = c(\"model\", \".value\"), names_pattern = \"(.*)_(.*)\") |&gt;\n    mutate(model = factor(model, levels = c(\"km\", \"cox\", \"aft\"))) |&gt;\n    # scatter plot with Pearson's coef.\n    ggpubr::ggscatter(\n      x = \"proper\", y = \"improper\",\n      facet.by = c(\"model\"),\n      panel.labs = list(model = c(\"Kaplan-Meier\", \"CoxPH\", \"AFT (Weibull)\")),\n      xlab = \"RISBS (proper)\",\n      ylab = \"ISBS (improper)\",\n      color = \"black\", shape = 21, size = 2,\n      add = \"reg.line\",  # Add regression line\n      add.params = list(color = \"blue\", fill = \"lightgray\"), # Customize regr. line\n      conf.int = TRUE, # Add confidence interval\n      cor.coef = TRUE, # Add Pearson's correlation coefficient\n      cor.coeff.args = list(method = \"pearson\", label.sep = \"\\n\")\n    ) +\n    labs(title = id) +\n    theme(panel.spacing = unit(1, \"cm\"))\n    print(p)\n}"
  },
  {
    "objectID": "index.html#investigate-inflation-of-proper-isbs",
    "href": "index.html#investigate-inflation-of-proper-isbs",
    "title": "Benchmark results analysis",
    "section": "Investigate inflation of proper ISBS",
    "text": "Investigate inflation of proper ISBS\n\n\n\n\n\n\nNote\n\n\n\nIn this section we investigate an example where the proper ISBS gets inflated (i.e. too large value for the score, compared to the improper version) and show how we can avoid such a thing from happening when evaluating model performance.\n\n\nLet’s use a dataset where in a particular train/test resampling the issue occurs:\n\n\nCode\ninflated_data = readRDS(file = \"inflated_data.rds\")\ntask = inflated_data$task\npart = inflated_data$part\n\ntask\n\n\n&lt;TaskSurv:mgus&gt; (176 x 9)\n* Target: time, status\n* Properties: -\n* Features (7):\n  - dbl (6): age, alb, creat, dxyr, hgb, mspike\n  - fct (1): sex\n\n\nSeparate train and test data:\n\n\nCode\ntask_train = task$clone()$filter(rows = part$train)\ntask_test  = task$clone()$filter(rows = part$test)\n\n\nKaplan-Meier of the training survival data:\n\n\nCode\nautoplot(task_train) +\n  labs(title = \"Kaplan-Meier (train data)\",\n       subtitle = \"Time-to-event distribution\")\n\n\n\n\n\nKaplan-Meier of the training censoring data:\n\n\nCode\nautoplot(task_train, reverse = TRUE) +\n    labs(title = \"Kaplan-Meier (train data)\",\n         subtitle = \"Censoring distribution\")\n\n\n\n\n\nEstimates of the censoring distribution G_{KM}(t) (values from the above figure):\n\n\nCode\nkm_train = task_train$kaplan(reverse = TRUE)\nkm_tbl = tibble(time = km_train$time, surv = km_train$surv)\ntail(km_tbl)\n\n\n# A tibble: 6 × 2\n   time  surv\n  &lt;dbl&gt; &lt;dbl&gt;\n1 12140 0.75 \n2 12313 0.625\n3 12319 0.5  \n4 12349 0.25 \n5 12689 0.125\n6 13019 0    \n\n\n\n\n\n\n\n\nImportant\n\n\n\nAs we can see from the above figures and table, due to having at least one censored observation at the last time point, G_{KM}(t_{max}) = 0 for t_{max} = 13019.\n\n\nIs there an observation on the test set that has died (status = 1) on that last time point (or after)?\n\n\nCode\nmax_time = max(km_tbl$time) # max time point\n\ntest_times  = task_test$times()\ntest_status = task_test$status()\n\n# get the id of the observation in the test data\nid = which(test_times &gt;= max_time & test_status == 1)\nid\n\n\n[1] 14\n\n\nYes there is such observation!\nIn mlr3proba using proper = TRUE for the RISBS calculation, this observation will be weighted by 1/0 according to the formula. Practically, to avoid division by zero, a small value eps = 0.001 will be used.\nLet’s train a simple Cox model on the train set and calculate its predictions on the test set:\n\n\nCode\ncox = lrn(\"surv.coxph\")\np = cox$train(task, part$train)$predict(task, part$test)\n\n\nWe calculate the ISBS (improper) and RISBS (proper) scores:\n\n\nCode\ngraf_improper = msr(\"surv.graf\", proper = FALSE, id = \"graf.improper\")\ngraf_proper   = msr(\"surv.graf\", proper = TRUE,  id = \"graf.proper\")\np$score(graf_improper, task = task, train_set = part$train)\n\n\ngraf.improper \n    0.1493429 \n\n\nCode\np$score(graf_proper  , task = task, train_set = part$train)\n\n\ngraf.proper \n   10.64584 \n\n\nAs we can see there is huge difference between the two versions of the score. We check the per-observation scores (integrated across all time points):\nObservation-wise RISBS scores:\n\n\nCode\ngraf_proper$scores\n\n\n [1]   0.08994417   0.02854219   0.04214266   0.15578719   0.05364692\n [6]   0.12969150   0.06463256   0.32033549   2.43262450   0.11602432\n[11]   0.03228501   0.10172088   0.14652850 367.10227335   0.18004727\n[16]   0.21991511   0.09070024   0.03507389   0.19856844   0.07925747\n[21]   0.07732517   0.06982001   0.19468406   0.05267402   0.02419841\n[26]   0.17645640   0.07633691   0.04379196   0.07839955   0.06684222\n[31]   0.05457688   0.02874430   0.04071108   0.00000000   0.00000000\n\n\nObservation-wise ISBS scores:\n\n\nCode\ngraf_improper$scores\n\n\n [1] 0.08994417 0.02854219 0.04214266 0.15578719 0.05364692 0.12969150\n [7] 0.06463256 0.32033549 0.62971109 0.11602432 0.03228501 0.10172088\n[13] 0.14652850 1.07969258 0.16743979 0.21991511 0.09070024 0.03507389\n[19] 0.19856844 0.07925747 0.07732517 0.06982001 0.19468406 0.05267402\n[25] 0.02419841 0.16199516 0.07633691 0.04379196 0.07839955 0.06684222\n[31] 0.05457688 0.02874430 0.04071108 0.03512466 0.46541333\n\n\nIt is the one observation that we identified earlier that causes the inflation of the RISBS score - it’s pretty much an outlier compared to all other values:\n\n\nCode\ngraf_proper$scores[id]\n\n\n[1] 367.1023\n\n\nBy setting t_max (time horizon to evaluate the measure up to) to the 95\\% quantile of the event times, we can solve the inflation problem of the proper RISBS score, since we will divide by a value larger than zero from the above table of G_{KM}(t) values. The t_max time point is:\n\n\nCode\nt_max = as.integer(quantile(task_train$unique_event_times(), 0.95))\nt_max\n\n\n[1] 10080\n\n\nIntegrating up to t_max, the proper RISBS score is:\n\n\nCode\ngraf_proper_tmax = msr(\"surv.graf\", proper = TRUE, t_max = t_max)\np$score(graf_proper_tmax, task = task, train_set = part$train) # ISBS\n\n\nsurv.graf \n0.1436484 \n\n\nThe score for the specific observation that had experienced the event at (or beyond) the latest training time point is now:\n\n\nCode\ngraf_proper_tmax$scores[id]\n\n\n[1] 0.141502\n\n\n\n\n\n\n\n\nSuggestion when calculating time-integrated scoring rules\n\n\n\nTo avoid the inflation of RISBS and generally have a more robust estimation of both RISBS and ISBS scoring rules, we advise to set the t_max argument (time horizon). This can be either study-driven or based on a meaningful quantile of the distribution of (usually event) times in your dataset (e.g. 80\\%)."
  }
]